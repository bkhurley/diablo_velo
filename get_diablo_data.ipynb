{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data from Strava API\n",
    "I obtain data from Strava on all cycling efforts on the segment [Diablo - North Gate to Summit](https://www.strava.com/segments/656860), a popular Bay Area road cycling segment that ascends the north side of Mount Diablo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import requisit libraries\n",
    "import requests\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "#import pandas as pd\n",
    "from strava_api import access_token # my Strava API credentials\n",
    "#import psycopg2 as pg2\n",
    "#from postgres_login import secret # my postgreSQL password\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data\n",
    "I use Strava's API to obtain data on the Mt Diablo south gate-to-summit segment. See details about Strava's authentication protocol [here](https://strava.github.io/api/v3/oauth/). \n",
    "\n",
    "For security reasons, I keep my API access token hidden. Anyone wishing to reproduce this analysis should create a file within this working directory called `strava_api.py` and within that file populate a string variable `access_token` with their own access token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# populate some general parameters\n",
    "extra_headers = {'Authorization' : 'Bearer %s' % access_token}\n",
    "api_base_url = 'https://www.strava.com/api/v3/'\n",
    "api_segment_url = api_base_url + 'segments/%d'\n",
    "api_segment_all_efforts_url = api_segment_url + '/all_efforts'\n",
    "\n",
    "per_page = 200 # Strava max\n",
    "\n",
    "segment_id = 656860 # \"Diablo - North Gate to Summit\"\n",
    "\n",
    "# for writing data to file\n",
    "segment_fname = 'data/raw_data/segment_%d.p' % segment_id\n",
    "all_efforts_fname = 'data/raw_data/all_efforts_%d.p' % segment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Data\n",
    "First, I'll get data on the segment itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some data on the segment itself\n",
    "# if we already have segment data on disk, unpickle it. otherwise, get it from Strava's API and pickle it.\n",
    "if os.path.isfile(segment_fname):\n",
    "    sys.stdout.write('Unpickling segment data %s\\n' % segment_fname)\n",
    "    segment_r = pickle.load(open(segment_fname, 'rb'))\n",
    "else:\n",
    "    segment_r = requests.get(api_segment_url % segment_id, headers=extra_headers).json()\n",
    "    # save segment data\n",
    "    pickle.dump(segment_r, open(segment_fname, 'wb'))\n",
    "# how many efforts have been made as of data retrieval date?\n",
    "n_efforts = segment_r['effort_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Efforts\n",
    "Strava returns a max of 200 efforts per \"page,\" but there are over 23,000 efforts on this segment. Thus, I iterate through and combine the pages of efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "all_efforts = []\n",
    "\n",
    "# do we want feedback on progress and elapsed time?\n",
    "verbose = 0 \n",
    "\n",
    "# if we already have the data pickled, load the file. otherwise, request effort data from Strava API \n",
    "if os.path.isfile(all_efforts_fname):\n",
    "    sys.stdout.write('Unpickling all efforts from %s\\n' % all_efforts_fname)\n",
    "    all_efforts = pickle.load(open(all_efforts_fname, 'rb'))\n",
    "else:\n",
    "    for i in range(1, round(2 + n_efforts / per_page)):\n",
    "        if verbose:\n",
    "            sys.stdout.write('Requesting page %d\\n' % i)\n",
    "        r = requests.get(api_segment_all_efforts_url % segment_id, headers=extra_headers, \n",
    "                         params={'per_page' : per_page, 'page' : i})\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            sys.stderr.write('Error, received code %d for summary request %d\\n' % \n",
    "                             (r.status_code, i))\n",
    "        else:\n",
    "            all_efforts.extend(r.json())  \n",
    "    \n",
    "    # how much time elapsed (sec) during the loop?\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    if verbose:\n",
    "        sys.stdout.write('\\nTotal time elapsed (seconds): %f\\n' % elapsed)\n",
    "\n",
    "    # save effort data\n",
    "    pickle.dump(all_efforts, open(all_efforts_fname, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Weather Data\n",
    "Let's scrape temperature, precipitation, and wind data from Weather Underground's weather history for Buchanan Airport (nearest Weather Almanac source to Diablo's north gate entrance). These weather features may affect cycling performance and thus may be useful to this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some functions for scraping tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to retern BeautifulSoup object from url\n",
    "def make_soup(url):\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soupdata = BeautifulSoup(page, 'html.parser')\n",
    "    return soupdata\n",
    "\n",
    "# function to build timestamp\n",
    "def make_timestamp(m=m, d=d, y=y):\n",
    "    # Format month\n",
    "    if len(str(m)) < 2:\n",
    "        mStamp = '0' + str(m)\n",
    "    else:\n",
    "        mStamp = str(m)\n",
    "    # Format day\n",
    "    if len(str(d)) < 2:\n",
    "        dStamp = '0' + str(d)\n",
    "    else:\n",
    "        dStamp = str(d)\n",
    "    # Build timestamp\n",
    "    time_stamp = str(y) + mStamp + dStamp\n",
    "    return time_stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively scrape weather history pages\n",
    "Iterate through Weather Underground's weather history page for each day within the date range (2006-2017). I use the `BeautifulSoup` library to parse the HTML and find the weather values of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname = \"/data/raw_data/wu_weather_history.csv\"\n",
    "\n",
    "if not os.path.isfile('/Users/bkhurley/git/diablo_velo' + output_fname):\n",
    "    \n",
    "    # initialize output file\n",
    "    output_file = open(output_fname, \"w\")\n",
    "    output_writer = csv.writer(output_file)\n",
    "    \n",
    "    # let's time this process\n",
    "    start = time.time()\n",
    "\n",
    "    # define the critical rows that we want from the table weather history page\n",
    "    out_cols = ['timestamp','Mean Temperature','Max Temperature','Min Temperature',\n",
    "                'Precipitation','Wind Speed','Max Wind Speed']\n",
    "    wu_rows = out_cols[1:]\n",
    "\n",
    "    # write feature names as first row in output file\n",
    "    output_writer.writerow(out_cols)\n",
    "\n",
    "    # Iterate through year, month, and day\n",
    "    for y in range(2006, 2018):\n",
    "        for m in range(1, 13):\n",
    "            for d in range(1, 32):\n",
    "                \n",
    "                # new row array for each day\n",
    "                row_array = []\n",
    "\n",
    "                # Check if leap year\n",
    "                if y%400 == 0:\n",
    "                    leap = True\n",
    "                elif y%100 == 0:\n",
    "                    leap = False\n",
    "                elif y%4 == 0:\n",
    "                    leap = True\n",
    "                else:\n",
    "                    leap = False\n",
    "\n",
    "                # Check if already gone through month\n",
    "                if (m == 2 and leap and d > 29):\n",
    "                    continue\n",
    "                elif (m == 2 and d > 28):\n",
    "                    continue\n",
    "                elif (m in [4, 6, 9, 10] and d > 30):\n",
    "                    continue\n",
    "\n",
    "                # Build timestamp & append to data row\n",
    "                timestamp = make_timestamp(m=m, d=d, y=y)\n",
    "                row_array.append(timestamp)\n",
    "\n",
    "                # Open wunderground.com url\n",
    "                url = (\"https://www.wunderground.com/history/airport/KCCR/\" +\n",
    "                       str(y)+ \"/\" + str(m) + \"/\" + str(d) + \"/DailyHistory.html\")\n",
    "\n",
    "                # get page content\n",
    "                soup = make_soup(url)\n",
    "\n",
    "                # Write loop to scrape each desired value and append to row.\n",
    "                # Loop through each weather output var, then loop through WU\n",
    "                # page rows to find value for that var\n",
    "                for record in soup.find_all('tr'):\n",
    "                    if record.find_all('td'):\n",
    "                        record_text = record.find_all('td')[0].text\n",
    "                        if record.find_all('td')[0].text in wu_rows:\n",
    "                            row_text = record.find_all('td')[1].text.split()\n",
    "                            if not record.find_all('td')[1].text.split():\n",
    "                                continue\n",
    "                            else:                            \n",
    "                                row_text = row_text[0]\n",
    "                            # deal with missing data\n",
    "                            if row_text in ['', '-']:\n",
    "                                row_array.append(float('nan'))\n",
    "                            else:\n",
    "                                # WU uses T to denote trace of participation. \n",
    "                                # I will treat that as 0.0 inches\n",
    "                                if row_text == 'T':\n",
    "                                    row_text = 0.0\n",
    "                                # convert str to numerical value and append to data row\n",
    "                                row_array.append(float(row_text))\n",
    "                # write a row of data for each effort\n",
    "                output_writer.writerow(row_array)\n",
    "\n",
    "            # update status as we work through each month/year\n",
    "            sys.stdout.write('Scraped weather records for Month: %d, Year: %d\\n' % (m, y))\n",
    "    \n",
    "    # Done getting data! Close file.\n",
    "    output_file.close()\n",
    "    sys.stdout.write('\\n\\nFinished scraping weather data! Data written to file: %s\\n' % output_fname)\n",
    "\n",
    "    # end the timer\n",
    "    end = time.time()\n",
    "    elapsed = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
